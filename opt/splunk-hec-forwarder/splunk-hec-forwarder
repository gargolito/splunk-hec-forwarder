#!/usr/bin/env python
# System modules
# need to implement http keep alive
# https://conf.splunk.com/files/2017/slides/measuring-hec-performance-for-fun-and-profit.pdf
# http://engineroom.ft.com/2016/11/03/splunk-http-event-collector-direct-pipe-to-splunk/
# these are supposed to help with cpu util.
from __future__ import print_function
from __future__ import division
import os, sys, Queue, threading , time, argparse, select, requests, urllib, json, time, socket, uuid, logging, ConfigParser
requests.packages.urllib3.disable_warnings()

_LOG_LEVEL_STRINGS = ['CRITICAL', 'ERROR', 'WARNING', 'INFO', 'DEBUG']

# this is for script debug/logging max nbr of messages that are processed within one wbatch
def _log_level_string_to_int(log_level_string):
    if not log_level_string in _LOG_LEVEL_STRINGS:
        message = 'invalid choice: {0} (choose from {1})'.format(log_level_string, _LOG_LEVEL_STRINGS)
        raise argparse.ArgumentTypeError(message)
    log_level_int = getattr(logging, log_level_string, logging.DEBUG)
    # check the logging log_level_choices have not changed from our expected values
    assert isinstance(log_level_int, int)
    return log_level_int

# begin defaults if config file not found.
pollPeriod = .05 # the number of seconds between polling for new messages
host = socket.gethostname()
default_log_name = "dev-splunk_hec_forwarder-" + host + ".log"

parser = argparse.ArgumentParser()
parser.add_argument("-t", "--token", help = "http event collector token", required = True)
parser.add_argument("-s", "--server", help = "http event collector fqdn", required = True)
parser.add_argument('--host',default = host)
parser.add_argument('-p','--port', help = "port",default = '8088')
parser.add_argument('--ssl', help = "use ssl",action = 'store_true',default = False)
parser.add_argument('--ssl_noverify', help = "disable ssl validation",action = 'store_false',default = True)
parser.add_argument('--source',default = "hec:syslog:" + host)
parser.add_argument('--sourcetype',default = "syslog")
parser.add_argument('--index',default = "main")
parser.add_argument('--maxBatch',help = "max number of records allowed in one batch of requests for hec",default = 10,type = int)
parser.add_argument('--maxQueue',help = "max number of records to be read from rsyslog queued for transfer",default = 5000,type = int)
parser.add_argument('--maxThreads',help = "max number of threads for work",default = 10,type = int)
parser.add_argument('--nopost', help = "don't post for debug reasons",action = 'store_false')
parser.add_argument('--log_dir',help = "log directory",default = "/var/log/",type = str )
parser.add_argument('--log_name',help = "log name",default = default_log_name,type = str )
parser.add_argument('--log_level', default = 'CRITICAL', dest = 'log_level', type = _log_level_string_to_int, nargs = '?', help = 'Set the logging output level. {0}'.format(_LOG_LEVEL_STRINGS))
args = parser.parse_args()
if args.log_name:
	if not ".log" in args.log_name:
		args.log_name = args.log_name + ".log"

#args.log_name = "splunk_hec_forwarder-" + host + ".log"

# end defaults if config file not found.

logger = logging.getLogger('splunk_hec_forwarder')
logger.setLevel(args.log_level)
fi = logging.FileHandler(os.path.join(args.log_dir,args.log_name))
fi.setLevel(args.log_level)
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
fi.setFormatter(formatter)
logger.addHandler(fi)
def onReceive(threadID, mq, tq, evt):
	"""This is the entry point where actual work needs to be done. It receives
	   a list with all messages pulled from rsyslog. The list is of variable
	   length, but contains all messages that are currently available. It is
	   suggest NOT to use any further buffering, as we do not know when the
	   next message will arrive. It may be in a nanosecond from now, but it
	   may also be in three hours...
	"""
	with requests.Session() as s:
		headers = {'Authorization':'Splunk '+args.token}
		session_id = str(uuid.uuid1())
		if args.ssl:
			protocol = 'https'
		else:
			protocol = 'http'
		querystring = urllib.urlencode({'channel' : session_id, 'source' : args.source, 'sourcetype' : args.sourcetype, 'index' : args.index, 'host': args.host  }) 
		server_uri = '%s://%s:%s/services/collector/raw?%s' % (protocol, args.server, args.port, querystring)
		logger.info('TheadID %s' % (threadID))
		logger.info('server_uri =  %s' % (server_uri))
		#print(server_uri + " token = " + args.token)
		#print('%s: starting, control state %s' % (threadID, evt.is_set()))
		while not evt.is_set() or not mq.empty():
			c = 0
			data = []
			sl = 0
			while sl<2 and c < args.maxBatch:
				try:
					m = mq.get(True, 1)
                                        if m:
                                            logger.debug('m =  %s' % (m))
                                            c = c+1
                                  #          print("###############top##############")
                                   #         print(m)
                                    #        print("#############bottom#############")
                                            data.append(m)
                                            mq.task_done()
				except Queue.Empty:
					sl = sl+1
					logger.debug("empty queue sleeping")
                                        # don't sleep so long [was time.sleep(3)]
					time.sleep(.002)
			#Place one event int the batch
                       # print("###############data#############")
                       # print(data)
                       # print("///////////////data/////////////")
			if c>0:
				logger.info("sending %s" %(c))
				d = "".join(data)
				if args.nopost:
					r = s.post(server_uri,data = d,headers = headers, verify = args.ssl_noverify)
					if r.status_code == 200:
						logger.debug("r.status_code = %s" % (r.status_code))
						logger.debug("r = %s" % (r.text))
					else:
						logger.warning("r.status_code = %s" % (r.status_code))
						logger.warning("r = %s" % (r.text))
			else:
				logger.info("sending 0")
		z = tq.get()
		#print '%s: Flushed Batch' % threadID
		tq.task_done()
"""
-------------------------------------------------------
This is plumbing that DOES NOT need to be CHANGED
-------------------------------------------------------
Implementor's note: Python seems to very agressively
buffer stdouot. The end result was that rsyslog does not
receive the script's messages in a timely manner (sometimes
even never, probably due to races). To prevent this, we
flush stdout after we have done processing. This is especially
important once we get to the point where the plugin does
two-way conversations with rsyslog. Do NOT change this!
See also: https://github.com/rsyslog/rsyslog/issues/22
"""

# the syslog listener
UDP_IP_ADDRESS = "127.0.0.1"
UDP_PORT_NO = 55514
# listening serverSocket
serverSock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
# bind serverSocket
serverSock.bind((UDP_IP_ADDRESS, UDP_PORT_NO))

stop_event = threading.Event()
#stop_event.set()
maxAtOnce = args.maxBatch
msgQueue = Queue.Queue(maxsize = args.maxQueue)
threadQueue = Queue.Queue(maxsize = args.maxThreads)
for i in range(args.maxThreads):
	threadQueue.put(i)
	worker = threading.Thread(target = onReceive, args = (i, msgQueue, threadQueue, stop_event))
    	worker.setDaemon(True)
	worker.start()
while not stop_event.is_set():
	while not stop_event.is_set() and serverSock:
# disabling polling because we're missing some events.            
#		time.sleep(pollPeriod)
# increase buffer size to see if that prevents script from missing events
		line, addr = serverSock.recvfrom(8192)
		if line:
			msgQueue.put(line)
			logger.info(line)
		else: # an empty line means stdin has been closed
			logger.info('end of stdin')
			stop_event.set()
			msgQueue.join()
			logger.info('msgQueue joined')
logger.info('waiting for thread shutdown')
threadQueue.join()
sys.stdout.flush() # very important, Python buffers far too much!
